{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:48:58.358526Z",
     "start_time": "2024-06-15T10:48:57.628334Z"
    }
   },
   "source": [
    "import glob \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:48:58.992083Z",
     "start_time": "2024-06-15T10:48:58.359805Z"
    }
   },
   "source": [
    "import torchaudio\n",
    "import librosa\n",
    "from torchaudio import transforms\n",
    "from model import WaveNet"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:48:59.171226Z",
     "start_time": "2024-06-15T10:48:58.994359Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:48:59.179140Z",
     "start_time": "2024-06-15T10:48:59.172505Z"
    }
   },
   "source": [
    "class VCTK(Dataset):\n",
    "    def __init__(self,path='./VCTK/',speaker='p225',transform=None,sr=16000,top_db=10):\n",
    "        self.wav_list = glob.glob(path + speaker +'/*.wav')\n",
    "        self.wav_ids = sorted([f.split('/')[-1] for f in glob.glob(path+'*')])\n",
    "        self.transform = transform\n",
    "        self.sr = sr\n",
    "        self.top_db = top_db\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        f = self.wav_list[index]\n",
    "        audio,_ = librosa.load(f,sr=self.sr,mono=True)\n",
    "        audio,_ = librosa.effects.trim(audio, top_db=self.top_db, frame_length=2048)\n",
    "        audio = np.clip(audio,-1,1)\n",
    "        wav_tensor = torch.from_numpy(audio).unsqueeze(1)\n",
    "        wav_id = f.split('/')[3]\n",
    "        if self.transform is not None:\n",
    "            wav_tensor = self.transform(wav_tensor)\n",
    "        \n",
    "        return wav_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wav_list)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:48:59.339832Z",
     "start_time": "2024-06-15T10:48:59.180990Z"
    }
   },
   "source": [
    "t = transforms.Compose([\n",
    "        transforms.MuLawEncoding(),\n",
    "        transforms.LC2CL()])\n",
    "\n",
    "def collate_fn_(batch_data, max_len=40000):\n",
    "    audio = batch_data[0]\n",
    "    audio_len = audio.size(1)\n",
    "    if audio_len > max_len:\n",
    "        idx = random.randint(0,audio_len - max_len)\n",
    "        return audio[:,idx:idx+max_len]\n",
    "    else:\n",
    "        return audio"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchaudio.transforms' has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-f5bbefa44da4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m t = transforms.Compose([\n\u001B[0m\u001B[1;32m      2\u001B[0m         \u001B[0mtransforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMuLawEncoding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         transforms.LC2CL()])\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcollate_fn_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_len\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m40000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torchaudio.transforms' has no attribute 'Compose'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "vctk = VCTK(speaker='p225',transform=t,sr=16000)\n",
    "training_data = DataLoader(vctk,batch_size=1, shuffle=True,collate_fn=collate_fn_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model = WaveNet().cuda()\n",
    "train_step = optim.Adam(model.parameters(),lr=2e-3, eps=1e-4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "scheduler = optim.lr_scheduler.MultiStepLR(train_step, milestones=[50,150,250], gamma=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "for epoch in range(1000):\n",
    "    loss_= []\n",
    "    scheduler.step()\n",
    "    for data in training_data:\n",
    "        \n",
    "        data = Variable(data).cuda()\n",
    "        x = data[:,:-1]\n",
    "        \n",
    "        logits = model(x)\n",
    "        y = data[:,-logits.size(2):]\n",
    "        loss = F.cross_entropy(logits.transpose(1,2).contiguous().view(-1,256), y.view(-1))\n",
    "        train_step.zero_grad()\n",
    "        loss.backward()\n",
    "        train_step.step()\n",
    "        loss_.append(loss.data[0])\n",
    "    \n",
    "    print epoch,np.mean(loss_)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
